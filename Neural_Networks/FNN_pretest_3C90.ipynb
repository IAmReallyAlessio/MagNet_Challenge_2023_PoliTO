{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"id":"ae8B3SjMQupp","outputId":"ade589e9-68e7-44f8-ec53-b2e68f44e54a","executionInfo":{"status":"error","timestamp":1699198391398,"user_tz":-60,"elapsed":4086,"user":{"displayName":"Simone Morra","userId":"13236581937281332325"}}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-350f8af812bf>\u001b[0m in \u001b[0;36m<cell line: 218>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-350f8af812bf>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0mtesting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-350f8af812bf>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Load .json Files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Dataset_tri_3C90.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mload_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mDATA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Dataset_tri_3C90.json'"]}],"source":["import torch\n","from torch import Tensor\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import random\n","import numpy as np\n","import json\n","import math\n","import matplotlib.pyplot as plt\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # Define a fully connected layers model with three inputs (frequency, flux density, duty ratio) and one output (core loss).\n","        self.layers = nn.Sequential(\n","            nn.Linear(4, 10),\n","            nn.ReLU(),\n","            nn.Linear(10, 20),\n","            nn.ReLU(),\n","            nn.Linear(20, 20),\n","            nn.ReLU(),\n","            nn.Linear(20, 10),\n","            nn.ReLU(),\n","            nn.Linear(10, 1),\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","def get_dataset():\n","    # Load .json Files\n","    with open('/Dataset_tri_3C90.json','r') as load_f:\n","        DATA = json.load(load_f)\n","\n","    Freq = DATA['Frequency']\n","    Flux = DATA['Flux_Density']\n","    Duty = DATA['Duty_Ratio']\n","    Temperature = DATA['Temperature']\n","    Power = DATA['Power_Loss']\n","\n","    # Compute labels\n","    # There's approximalely an exponential relationship between Loss-Freq and Loss-Flux.\n","    # Using logarithm may help to improve the training.\n","    Freq = np.log10(Freq)\n","    Flux = np.log10(Flux)\n","    Duty = np.array(Duty)\n","    Temperature = np.array(Temperature)\n","    Power = np.log10(Power)\n","\n","    # Reshape data\n","    Freq = Freq.reshape((-1,1))\n","    Flux = Flux.reshape((-1,1))\n","    Duty = Duty.reshape((-1,1))\n","    Temperature = Temperature.reshape((-1,1))\n","\n","    print(np.shape(Freq))\n","    print(np.shape(Flux))\n","    print(np.shape(Duty))\n","    print(np.shape(Temperature))\n","    print(np.shape(Power))\n","\n","    temp = np.concatenate((Freq,Flux,Duty, Temperature),axis=1) #Temperature\n","\n","    in_tensors = torch.from_numpy(temp).view(-1, 4)\n","    out_tensors = torch.from_numpy(Power).view(-1, 1)\n","\n","    # # Save dataset for future use\n","    # np.save(\"dataset.fc.in.npy\", in_tensors.numpy())\n","    # np.save(\"dataset.fc.out.npy\", out_tensors.numpy())\n","\n","    return torch.utils.data.TensorDataset(in_tensors, out_tensors)\n","\n","def get_test():\n","    with open('/Dataset_tri_3C90_pretest.json','r') as load_test:\n","        DATA_TEST = json.load(load_test)\n","\n","    Freq_t = DATA_TEST['Frequency']\n","    Flux_t = DATA_TEST['Flux_Density']\n","    Duty_t = DATA_TEST['Duty_Ratio']\n","    Temperature_t = DATA_TEST['Temperature']\n","    Power_t = DATA_TEST['Power_Loss']\n","\n","    # Compute labels\n","    # There's approximalely an exponential relationship between Loss-Freq and Loss-Flux.\n","    # Using logarithm may help to improve the training.\n","    Freq_t = np.log10(Freq_t)\n","    Flux_t = np.log10(Flux_t)\n","    Duty_t = np.array(Duty_t)\n","    Temperature_t = np.array(Temperature_t)\n","    Power_t = np.log10(Power_t)\n","\n","    # Reshape data\n","    Freq_t = Freq_t.reshape((-1,1))\n","    Flux_t = Flux_t.reshape((-1,1))\n","    Duty_t = Duty_t.reshape((-1,1))\n","    Temperature_t = Temperature_t.reshape((-1,1))\n","\n","    tmp = np.concatenate((Freq_t, Flux_t, Duty_t, Temperature_t),axis=1)\n","\n","    in_tensors = torch.from_numpy(tmp).view(-1, 4)\n","    out_tensors = torch.from_numpy(Power_t).view(-1, 1)\n","\n","    return torch.utils.data.TensorDataset(in_tensors, out_tensors)\n","\n","\n","def main():\n","    # Reproducibility\n","    random.seed(1)\n","    np.random.seed(1)\n","    torch.manual_seed(1)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","    # Hyperparameters\n","    NUM_EPOCH = 3000\n","    BATCH_SIZE = 128\n","    DECAY_EPOCH = 150\n","    DECAY_RATIO = 0.7\n","    LR_INI = 0.01\n","\n","    # Select GPU as default device\n","    device = torch.device(\"cuda\")\n","\n","    # Load dataset\n","    dataset = get_dataset()\n","    testing = get_test()\n","\n","    # Split the dataset\n","    train_size = int(0.7 * len(dataset))\n","    valid_size = len(dataset)-train_size\n","    test_size = int(len(testing))\n","    train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n","    test_dataset = testing\n","    kwargs = {'num_workers': 0, 'pin_memory': True, 'pin_memory_device': \"cuda\"}\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n","    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n","    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n","\n","    # Setup network\n","    net = Net().double().to(device)\n","\n","    # Log the number of parameters\n","    print(\"Number of parameters: \", count_parameters(net))\n","\n","    # Setup optimizer\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(net.parameters(), lr=LR_INI)\n","\n","    # Train the network\n","    for epoch_i in range(NUM_EPOCH):\n","\n","        # Train for one epoch\n","        epoch_train_loss = 0\n","        net.train()\n","        optimizer.param_groups[0]['lr'] = LR_INI* (DECAY_RATIO ** (0+ epoch_i // DECAY_EPOCH))\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = net(inputs.to(device))\n","            loss = criterion(outputs, labels.to(device))\n","            loss.backward()\n","            optimizer.step()\n","\n","            epoch_train_loss += loss.item()\n","\n","        # Compute Validation Loss\n","        with torch.no_grad():\n","            epoch_valid_loss = 0\n","            for inputs, labels in valid_loader:\n","                outputs = net(inputs.to(device))\n","                loss = criterion(outputs, labels.to(device))\n","\n","                epoch_valid_loss += loss.item()\n","\n","        if (epoch_i+1)%200 == 0:\n","          print(f\"Epoch {epoch_i+1:2d} \"\n","              f\"Train {epoch_train_loss / len(train_dataset) * 1e5:.5f} \"\n","              f\"Valid {epoch_valid_loss / len(valid_dataset) * 1e5:.5f}\")\n","\n","    # Save the model parameters\n","    torch.save(net.state_dict(), \"/Model_FNN.sd\")\n","    print(\"Training finished! Model is saved!\")\n","\n","\n","    # Evaluation\n","    net.eval()\n","    y_meas = []\n","    y_pred = []\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            y_pred.append(net(inputs.to(device)))\n","            y_meas.append(labels.to(device))\n","\n","    y_meas = torch.cat(y_meas, dim=0)\n","    y_pred = torch.cat(y_pred, dim=0)\n","    print(f\"Test Loss: {F.mse_loss(y_meas, y_pred).item() / len(test_dataset) * 1e5:.5f}\")\n","\n","    yy_pred = 10**(y_pred.cpu().numpy())\n","    yy_meas = 10**(y_meas.cpu().numpy())\n","\n","    # Relative Error\n","    Error_re = abs(yy_pred-yy_meas)/abs(yy_meas)*100\n","    Error_re_avg = np.mean(Error_re)\n","    Error_re_rms = np.sqrt(np.mean(Error_re ** 2))\n","    Error_re_max = np.max(Error_re)\n","    print(f\"Relative Error: {Error_re_avg:.8f}\")\n","    print(f\"RMS Error: {Error_re_rms:.8f}\")\n","    print(f\"MAX Error: {Error_re_max:.8f}\")\n","    plt.hist(Error_re, bins = 30)\n","    plt.show()\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}]}